[{"id":0,"href":"/getstarted/get-started-cluster/","title":"Your First Oakestra Cluster","parent":"Get Started","content":"\nTable of content:\nHigh-level archtecture Create your first Oakestra cluster High-level architecture Oakestra lets you deploy your workload on devices of any size. From a small RasperryPi to a cloud instance far away on GCP or AWS. The tree structure enables you to create multiple clusters of resources.\nThe Root Orchestrator manages different clusters of resources. The root only sees aggregated cluster resources. The Cluster orchestrator manages your worker nodes. This component collects the real-time resources and schedules your workloads to the perfect matching device. A Worker is any device where a component called NodeEngine is installed. Each node can support multiple execution environments such as Containers (containerd runtime), MicroVM (containerd runtime), and Unikernels (mirageOS). Disclaimer, currently, only containers are supported. Help is still needed for Unikernels and MicroVMs.\nCreate your first Oakestra cluster Let\u0026rsquo;s start simple with a single node deployment, where all the components are in the same device. Then, we will separate the components and use multiple devices until we\u0026rsquo;re able to create multiple clusters.\nRequirements: Linux (Workers only) Docker + Docker compose (Orchestrators only) Cluster Orchestrator and Root Orchestrator machines must be mutually reachable. 1-DOC (1 Device, One Cluster) In this example, we will use a single device to deploy all the components. This is not recommended for production environments, but it is pretty cool for home environments and development.\n0) First, let\u0026rsquo;s export the required environment variables\n## Choose a unique name for your cluster export CLUSTER_NAME=My_Awesome_Cluster ## Come up with a name for the current location export CLUSTER_LOCATION=My_Awesome_Apartment 1) now clone the repository and move into it using:\ngit clone https://github.com/oakestra/oakestra.git \u0026amp;\u0026amp; cd oakestra 2) Run a local 1-DOC cluster\nsudo -E docker-compose -f run-a-cluster/1-DOC.yml up 3) download, untar and install the node engine package\nwget -c https://github.com/oakestra/oakestra/releases/download/v0.4.2/NodeEngine_$(dpkg --print-architecture).tar.gz \u0026amp;\u0026amp; tar -xzf NodeEngine_$(dpkg --print-architecture).tar.gz \u0026amp;\u0026amp; chmod +x install.sh \u0026amp;\u0026amp; ./install.sh 4) (optional) download and unzip and install the network manager; this enables an overlay network across your services\nwget -c https://github.com/oakestra/oakestra-net/releases/download/v0.4.2/NetManager_$(dpkg --print-architecture).tar.gz \u0026amp;\u0026amp; tar -xzf NetManager_$(dpkg --print-architecture).tar.gz \u0026amp;\u0026amp; chmod +x install.sh \u0026amp;\u0026amp; ./install.sh $(dpkg --print-architecture) ( please replace \u0026lt; arch \u0026gt; with your device architecture: arm-7 or amd64 )\n4.1) Edit /etc/netmanager/netcfg.json as follows:\n{ \u0026#34;NodePublicAddress\u0026#34;: \u0026#34;\u0026lt;IP ADDRESS OF THIS DEVICE\u0026gt;\u0026#34;, \u0026#34;NodePublicPort\u0026#34;: \u0026#34;\u0026lt;PORT REACHABLE FROM OUTSIDE, use 50103 as default\u0026gt;\u0026#34;, \u0026#34;ClusterUrl\u0026#34;: \u0026#34;localhost\u0026#34;, \u0026#34;ClusterMqttPort\u0026#34;: \u0026#34;10003\u0026#34; } 4.2) start the NetManager on port 6000\nsudo NetManager -p 6000 \u0026amp; 5) start the NodeEngine. Please only use the -n 6000 parameter if you started the network component in step 4. This parameter, in fact, is used to specify the internal port of the network component, if any.\nsudo NodeEngine -n 6000 -p 10100 ( you can use NodeEngine -h for further details )\nM-DOC (M Devices, One Cluster) The M-DOC deployment enables you to deploy One cluster with multiple worker nodes. The main difference between this deployment and 1-DOC is that the worker nodes might be external here, and there can be multiple of them.\nThe deployment of this kind of cluster is similar to 1-DOC. We first need to start the root and cluster orchestrator. Afterward, we can attach the worker nodes.\n1) On the node you wish to use as a cluster and root orchestrator, execute steps 1-DOC.1 and 1-DOC.2\n2) Now, we need to prepare all the worker nodes. On each worker node, execute the following:\n2.1) Downlaod and unpack both the NodeEngine and the NetManager:\nwget -c https://github.com/oakestra/oakestra/releases/download/v0.4.2/NodeEngine_$(dpkg --print-architecture).tar.gz \u0026amp;\u0026amp; tar -xzf NodeEngine_$(dpkg --print-architecture).tar.gz \u0026amp;\u0026amp; chmod +x install.sh \u0026amp;\u0026amp; ./install.sh wget -c https://github.com/oakestra/oakestra-net/releases/download/v0.4.2/NetManager_$(dpkg --print-architecture).tar.gz \u0026amp;\u0026amp; tar -xzf NetManager_$(dpkg --print-architecture).tar.gz \u0026amp;\u0026amp; chmod +x install.sh \u0026amp;\u0026amp; ./install.sh $(dpkg --print-architecture) 2.2) Edit /etc/netmanager/netcfg.json accordingly:\n{ \u0026#34;NodePublicAddress\u0026#34;: \u0026#34;\u0026lt;IP ADDRESS OF THIS DEVICE\u0026gt;\u0026#34;, \u0026#34;NodePublicPort\u0026#34;: \u0026#34;\u0026lt;PORT REACHABLE FROM OUTSIDE, internal port is always 50103\u0026gt;\u0026#34;, \u0026#34;ClusterUrl\u0026#34;: \u0026#34;\u0026lt;IP ADDRESS OF THE CLSUTER ORCHESTRATOR\u0026gt;\u0026#34;, \u0026#34;ClusterMqttPort\u0026#34;: \u0026#34;10003\u0026#34; } 2.3) Run the NetManager and the NodeEngine components:\nsudo NetManager -p 6000 \u0026amp; sudo NodeEngine -n 6000 -p 10100 -a \u0026lt;IP ADDRESS OF THE CLSUTER ORCHESTRATOR\u0026gt; MDNC (M Devices, N Clusters) This represents the most versatile deployment. You can split your resources into multiple clusters within different locations and with different resources. In this deployment, we need to deploy the Root and the Cluster orchestrator on different nodes. Each independent clsuter orchestrator represents a cluster of resources. The worker nodes attached to each cluster are aggregated and seen as a unique big resource from the point of view of the Root. This deployment isolates the resources from the root perspective and delegates the responsibility to the cluster orchestrator. 1) In this first step, we need to deploy the RootOrchestrator component on a Node. To do this, you need to clone the repository on the desired node, move to the root orchestrator folder, and execute the startup command.\ngit clone https://github.com/edgeIO/edgeio.git \u0026amp;\u0026amp; cd edgeio sudo -E docker-compose -f root_orchestrator/docker-compose-\u0026lt;arch\u0026gt;.yml up ( please replace \u0026lt; arch \u0026gt; with your device architecture: arm or amd64 )\n2) For each node that needs to host a cluster orchestrator, you need to: 2.1) Export the ENV variables needed to connect to the cluster orchestrator:\nexport SYSTEM_MANAGER_URL=\u0026lt;IP ADDRESS OF THE NODE HOSTING THE ROOT ORCHESTRATOR\u0026gt; export CLUSTER_NAME=\u0026lt;choose a name for your cluster\u0026gt; export CLUSTER_LOCATION=\u0026lt;choose a name for the cluster\u0026#39;s location\u0026gt; 2.2) Clone the repo and run the cluster orchestrator:\ngit clone https://github.com/edgeIO/edgeio.git \u0026amp;\u0026amp; cd edgeio sudo -E docker-compose -f cluster_orchestrator/docker-compose-\u0026lt;arch\u0026gt;.yml up ( please replace \u0026lt; arch \u0026gt; with your device architecture: arm or amd64 )\n3) Start and configure each worker as described in M-DOC.2\nHybrids You should have got the gist now, but if you want, you can build the infrastructure by composing the components like LEGO blocks. Do you want to give your Cluster Orchestrator computational capabilities for the deployment? Deploy there the NodeEngine+Netmanager components, and you\u0026rsquo;re done. You don\u0026rsquo;t want to use a separate node for the Root Orchestrator? Simply deploy it all together with a cluster orchestrator.\n"},{"id":1,"href":"/getstarted/","title":"Get Started","parent":"","content":""},{"id":2,"href":"/getstarted/get-started-app/","title":"Deploy your first App","parent":"Get Started","content":"\nTable of content:\nRequirements Deploy your first applications Requirements You have a running Root Orchestrator with at least one Cluster Orchestrator registered. You have at least one Worker Node Registered (Optional) If you want the microservices to communicate, you need to have the NetManager installed and properly configured. You can access the APIs at \u0026lt;root-orch-ip\u0026gt;:10000/api/docs Deploy your first application Let\u0026rsquo;s try deploying an Nginx server and a client. Then we\u0026rsquo;ll enter inside the client container and try to curl Nginx.\nAll we need to do to deploy an application is to create a deployment descriptor and submit it to the platform using the APIs.\nDeployment descriptor In order to deploy a container a deployment descriptor must be passed to the deployment command. The deployment descriptor contains all the information that Oakestra needs in order to achieve a complete deploy in the system.\nSince version 0.4, Oakestra (previously, EdgeIO) uses the following deployment descriptor format.\ndeploy_curl_application.yaml\n{ \u0026#34;sla_version\u0026#34; : \u0026#34;v2.0\u0026#34;, \u0026#34;customerID\u0026#34; : \u0026#34;Admin\u0026#34;, \u0026#34;applications\u0026#34; : [ { \u0026#34;applicationID\u0026#34; : \u0026#34;\u0026#34;, \u0026#34;application_name\u0026#34; : \u0026#34;clientsrvr\u0026#34;, \u0026#34;application_namespace\u0026#34; : \u0026#34;test\u0026#34;, \u0026#34;application_desc\u0026#34; : \u0026#34;Simple demo with curl client and Nginx server\u0026#34;, \u0026#34;microservices\u0026#34; : [ { \u0026#34;microserviceID\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;microservice_name\u0026#34;: \u0026#34;curl\u0026#34;, \u0026#34;microservice_namespace\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;virtualization\u0026#34;: \u0026#34;container\u0026#34;, \u0026#34;cmd\u0026#34;: [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;tail -f /dev/null\u0026#34;], \u0026#34;memory\u0026#34;: 100, \u0026#34;vcpus\u0026#34;: 1, \u0026#34;vgpus\u0026#34;: 0, \u0026#34;vtpus\u0026#34;: 0, \u0026#34;bandwidth_in\u0026#34;: 0, \u0026#34;bandwidth_out\u0026#34;: 0, \u0026#34;storage\u0026#34;: 0, \u0026#34;code\u0026#34;: \u0026#34;docker.io/curlimages/curl:7.82.0\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;9080\u0026#34;, \u0026#34;added_files\u0026#34;: [] }, { \u0026#34;microserviceID\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;microservice_name\u0026#34;: \u0026#34;nginx\u0026#34;, \u0026#34;microservice_namespace\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;virtualization\u0026#34;: \u0026#34;container\u0026#34;, \u0026#34;cmd\u0026#34;: [], \u0026#34;memory\u0026#34;: 100, \u0026#34;vcpus\u0026#34;: 1, \u0026#34;vgpus\u0026#34;: 0, \u0026#34;vtpus\u0026#34;: 0, \u0026#34;bandwidth_in\u0026#34;: 0, \u0026#34;bandwidth_out\u0026#34;: 0, \u0026#34;storage\u0026#34;: 0, \u0026#34;code\u0026#34;: \u0026#34;docker.io/library/nginx:latest\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;6080:60/tcp\u0026#34;, \u0026#34;addresses\u0026#34;: { \u0026#34;rr_ip\u0026#34;: \u0026#34;10.30.30.30\u0026#34; }, \u0026#34;added_files\u0026#34;: [] } ] } ] } This deployment descriptor example generates one application named clientserver with the test namespace and two microservices:\nnginx server with test namespace, namely clientserver.test.nginx.test curl client with test namespace, namely clientserver.test.curl.test This is a detailed description of the deployment descriptor fields currently implemented:\nsla_version: the current version is v0.2 customerID: id of the user, default is Admin application list, in a single deployment descriptor is possible to define multiple applications, each containing:\nFully qualified app name: A fully qualified name in Oakestra is composed of application_name: unique name representing the application (max 10 char, no symbols) application_namespace: namespace of the app, used to reference different deployment of the same application. Examples of namespace name can be default or production or test (max 10 char, no symbols) applicationID: leave it empty for new deployments, this is needed only to edit an existing deployment. application_desc: Short description of the application microservice list, a list of the microservices composing the application. For each microservice the user can specify: microserviceID: leave it empty for new deployments, this is needed only to edit an existing deployment. Fully qualified service name: microservice_name: name of the service (max 10 char, no symbols) microservice_namespace: namespace of the service, used to reference different deployment of the same service. Examples of namespace name can be default or production or test (max 10 char, no symbols) virtualization: currently the only uspported virtualization is container cmd: list of the commands to be executed inside the container at startup vcpu,vgpu,memory: minimum cpu/gpu vcores and memory amount needed to run the container vtpus: currently not implemented storage: minimum storage size required (currently the scheduler does not take this value into account) bandwidth_in/out: minimum required bandwith on the worker node. (currently the scheduler does not take this value into account) port: port mapping for the container in the syntax hostport_1:containerport_1[/protocol];hostport_2:containerport_2[/protocol] (default protocol is tcp) addresses: allows to specify a custom ip address to be used to balance the traffic across all the service instances. rr_ip: [optional filed] This field allows you to setup a custom Round Robin network address to reference all the instances belonging to this service. This address is going to be permanently bounded to the service. The address MUST be in the form 10.30.x.y and must not collide with any other Instance Address or Service IP in the system, otherwise an error will be returned. If you don\u0026rsquo;t specify a RR_ip and you don\u0026rsquo;t set this field, a new address will be generated by the system. constraints: array of constraints regarding the service. type: constraint type direct: Send a deployment to a specific cluster and a specific list of eligible nodes. You can specify \u0026quot;node\u0026quot;:\u0026quot;node1;node2;...;noden\u0026quot; a list of node\u0026rsquo;s hostnames. These are the only eligible worker nodes. \u0026quot;cluster\u0026quot;:\u0026quot;cluster_name\u0026quot; The name of the cluster where this service must be scheduled. E.g.: \u0026#34;constraints\u0026#34;:[ { \u0026#34;type\u0026#34;:\u0026#34;direct\u0026#34;, \u0026#34;node\u0026#34;:\u0026#34;xavier1\u0026#34;, \u0026#34;cluster\u0026#34;:\u0026#34;gpu\u0026#34; } ] Login to the APIs After running a cluster you can use the debug OpenAPI page to interact with the apis and use the infrastructure\nconnect to \u0026lt;root_orch_ip\u0026gt;:10000/api/docs\nAuthenticate using the following procedure:\nlocate the login method and use the try-out button Use the default Admin credentials to login Copy the result login token Go to the top of the page and authenticate with this token Register an application and the services After you authenticate with the login function, you can try out to deploy the first application.\nUpload the deployment description to the system. You can try using the deployment descriptor above. The response contains the Application id and the id for all the application\u0026rsquo;s services. Now the application and the services are registered to the platform. It\u0026rsquo;s time to deploy the service instances!\nYou can always remove or create a new service for the application using the /api/services endpoints\nDeploy an instance of a registered service Trigger a deployment of a service\u0026rsquo;s instance using POST /api/service/{serviceid}/instance each call to this endpoint generates a new instance of the service\nMonitor the service status With GET /api/aplications/\u0026lt;userid\u0026gt; (or simply /api/aplications/ if you\u0026rsquo;re admin) you can check the list of the deployed application. With GET /api/services/\u0026lt;appid\u0026gt; you can check the services attached to an application With GET /api/service/\u0026lt;serviceid\u0026gt; you can check the status for all the instances of Undeploy Use DELETE /api/service/\u0026lt;serviceid\u0026gt; to delete all the instances of a service Use DELETE /api/service/\u0026lt;serviceid\u0026gt;/instance/\u0026lt;instance number\u0026gt; to delete a specific instance of a service Use DELETE /api/application/\u0026lt;appid\u0026gt; to delete all together an application with all the services and instances Check if the deployment succeded If both services show the status ACTIVE then everything went fine. Otherwise, there might be a configuration issue or a bug. Please debug it with docker logs system_manager -f --tail=100 on the root orchestrator and with docker logs cluster_manager -f --tail=100 on the cluster orchestrator and open an issue.\nIf both services are ACTIVE, it is time to test the communication.\nMove into the worker node hosting the client and use the following command to log into the container.\nsudo ctr -n edge.io task exec --exec-id term1 Client.default.client.default /bin/sh Once we are inside our client, we can curl the Nginx server and check if everything works.\ncurl 10.30.30.30 Note that this address is the one we specified in the Nginx\u0026rsquo;s deployment descriptor.\n"},{"id":3,"href":"/docs/","title":"Docs","parent":"","content":""},{"id":4,"href":"/post/","title":"News","parent":"","content":""},{"id":5,"href":"/tags/GetSarted/","title":"GetSarted","parent":"Tags","content":""},{"id":6,"href":"/tags/","title":"Tags","parent":"","content":""},{"id":7,"href":"/about/contacts/","title":"Contacts","parent":"About","content":" Our discussion group Join our github discussion group HERE\nFeel free to post questions and ideas or ask about any problem you have.\nIn our team section, you can also find a list of people you can contact directly by email, socials, and, why not, carrier pigeons as well.\n"},{"id":8,"href":"/about/team/","title":"Team","parent":"About","content":" Giovanni Bartolomeo giovanni.bartolomeo@tum.de Mehdi Yosofie Oliver Haluszczynsci Simon Bäurle Maximilian Eder Patrick Sabanic Sonia Klärmann Ralf Baun Daniel Mair Maria Vienalas Dr. Nitinder Mohan mohan@in.tum.de Prof. Jörg Ott "},{"id":9,"href":"/tags/Architecture/","title":"Architecture","parent":"Tags","content":""},{"id":10,"href":"/docs/contribute/coc/","title":"Contributor Covenant Code of Conduct","parent":"Contribute","content":" Contributor Covenant Code of Conduct Our Pledge In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to make participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.\nOur Standards Examples of behavior that contributes to creating a positive environment include:\nUsing welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include:\nThe use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others\u0026rsquo; private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Our Responsibilities Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.\nProject maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.\nScope This Code of Conduct applies within all project spaces, and it also applies when an individual is representing the project or its community in public spaces. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.\nEnforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at [INSERT EMAIL ADDRESS]. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.\nProject maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project\u0026rsquo;s leadership.\nAttribution This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\nFor answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq\n"},{"id":11,"href":"/docs/operations/application-deployment/","title":"Deploy your first application","parent":"Opeartions","content":" Documentation available soon. Stay tuned. "},{"id":12,"href":"/docs/oakestra/architecture/","title":"High level architecture","parent":"Concepts","content":"\nTable of content Root Orchestrator Cluster Orchestrator Worker Node Oakestra Detailed Architecture As shown in our Get Started guide, Oakestra uses 3-4 building blocks to operate.\nRoot Orchestrator Cluster Orchestrator Node Engine NetManager (optional) This section of the wiki is intended for people willing to contribute to the project and it is meant to describe some internal architectural details.\nRoot Orchestrator The Root Orchestrator is a centralized control plane that is aware of the participating clusters.\nThis picture describes the containers that compose the Root Orchestrator. As you may have seen we use docker-compose to bring up the orchestrators. This is because each block of this picture is currently a separated container.\nThe System Manager is the point of contact for users, developers, or operators to use the system as an application deployment platform. It exposes APIs to receive deployment commands from users (application management) and APIs to handle slave Cluster Orchestrators. Cluster Orchestrators send their information regularly, and the System Manager is aware of those clusters. The scheduler calculates a placement for a given application within the available clusters. Mongo is the interface we use to access the database. We store aggregated information about the participating clusters. We differentiate between static metadata and dynamic data. The former covers the IP address, port number, name, and location of each cluster. The latter can be data that is changing regularly, such as the number of worker nodes per cluster, total amount of CPU cores and memory size, total amount of disk space, GPU capabilities, etc. The Root Network Components are detailed in the Oakestra-Net Wiki. System Manager APIs TODO\nJobs DB Structure TODO\nClusters DB Structure TODO\nScheduler Algorithms TODO\nConsideration regarding failure and scalability: The main problem of a centralized control plane is that it can act as a single point of failure. By design without a Root Orchestrator, the clusters are able to satisfy the SLA for the deployed applications internally, the only affected functionalities are the deployment of new services and the intra-cluster migrations. To avoid failure and increase resiliency an idea is to make the component able to scale by introducing a load balancer in front of the replicated components. However, this feature is not implemented yet.\nCluster Orchestrator TODO\nCluster Manager APIs TODO\nMQTT Topics TODO\nJobs DB structure TODO\nNodes DB structure TODO\nSchedulers Algorithms TODO\nWorker Node TODO\n"},{"id":13,"href":"/docs/networking/oakestra-mesh/","title":"Oakestra mesh network","parent":"Networking","content":"#Docs coming soon, stay tuned\n"},{"id":14,"href":"/docs/oakestra/orchestrators/","title":"Orchestration","parent":"Concepts","content":"#APIs\nRoot Orchestrator APIs OpenApi Spec\n"},{"id":15,"href":"/docs/oakestra/scheduling/","title":"Task Scheduling","parent":"Concepts","content":" How does the scheduling work in Oakestra? Oakestra\u0026rsquo;s architecture is composed of two tiers. Resources are divided into clusters. A cluster is seen as the aggregation of all its resources. A job is first scheduled to a cluster, and then the cluster scheduler decides the target worker.\nThe scheduling component is as simple as a Celery worker. The scheduler receives a job description and gives back an allocation target. We differentiate between the Root scheduler and Cluster scheduler. The Root scheduler finds a suitable cluster (step 1), and the Cluster scheduler finds a suitable worker node (step 2).\nThis scheduling algorithm does not ensure an absolute optimal deployment but consistently reduces the search space.\nScheduling Algorithm At each layer, the scheduling decision consists of the creation of a candidate_list of clusters (or workers), the exclusion of unsuitable candidates, and then the selection of the \u0026ldquo;best\u0026rdquo; candidate accordingly to a scheduling algorithm.\nThe scheduling algorithms are implemented in the calculation.py component of each respective scheduler.\nThe current released version only implements a best fit and first fit calculation strategies. However, on its way to the release, we have our new LDP algorithm (check it out on our whitepaper).\nJob Constraints The Job deployment descriptor allows a developer to specify constraints of 4 types: node resources, network capabilities, geographical positioning, and direct mapping.\nResources The job resource requirements cause the immediate exclusion of a candidate from the candidate list. These resources represent the bare minimum required by the job to operate properly. Here there is a table of the supported resources and the state of development:\nResource type Status Comments Virtualization 🟢 Fully functional containers support. Unikernel support is under development. CPU 🟢 Only number of CPU cores Memory 🟢 Memory requirements in MB Storage 🟠 It is possible to specify it, but it is not yet taken into account by the scheduler GPU 🟠 Possibility of specifying the GPU cores. But not yet the available GPU drivers. Right now, the support is only for CUDA. TPU 🔴 Not yet under development Architecture 🔴 Not yet possible to filter out the architecture. With containers, it is possible to use the multi-platform build. This flag is coming out together with the Unikernel support. Network \u0026amp; Geo constraints The networking requirements selection and geographic constraints support are coming out in our next release v0.5 and are part of the LDP algorithm update. Stay tuned.\nDirect mapping positioning It is possible to specify a direct mapping constraint. Therefore, in the deployment description, a developer can specify a list of target clusters and nodes. The scheduling algorithm operates only on the active clusters (or nodes) among the given list.\nThis direct mapping approach is currently based on cluster names and worker hostnames. We are anyway considering adding a label-based positioning where it is possible to tag resources with custom-defined labels. Stay tuned for more.\n"},{"id":16,"href":"/tags/Welcome/","title":"Welcome","parent":"Tags","content":""},{"id":17,"href":"/post/welcome/","title":"Welcome to Oakestra","parent":"News","content":"Oakestra is a hierarchical, lightweight, flexible, and scalable orchestration framework for edge computing.\nThrough its novel federated cluster management, delegated task scheduling, and semantic overlay networking, Oakestra can flexibly consolidate multiple infrastructure providers and support applications over dynamic variations at the edge.\nOur comprehensive evaluation against the stateof-the-art demonstrates the significant benefits of Oakestra as it achieves approximately 10× resource usage reduction and 10% application performance improvement.\n"},{"id":18,"href":"/","title":"","parent":"","content":""},{"id":19,"href":"/tags/about/","title":"about","parent":"Tags","content":""},{"id":20,"href":"/about/","title":"About","parent":"","content":""},{"id":21,"href":"/docs/guicli/","title":"CLI \u0026 Frontend","parent":"Docs","content":""},{"id":22,"href":"/tags/concepts/","title":"concepts","parent":"Tags","content":""},{"id":23,"href":"/docs/oakestra/","title":"Concepts","parent":"Docs","content":""},{"id":24,"href":"/docs/contribute/","title":"Contribute","parent":"Docs","content":""},{"id":25,"href":"/tags/docs/","title":"docs","parent":"Tags","content":""},{"id":26,"href":"/tags/getstarted/","title":"getstarted","parent":"Tags","content":""},{"id":27,"href":"/docs/networking/","title":"Networking","parent":"Docs","content":""},{"id":28,"href":"/docs/operations/","title":"Opeartions","parent":"Docs","content":""},{"id":29,"href":"/tags/operations/","title":"operations","parent":"Tags","content":""}]